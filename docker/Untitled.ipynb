{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915dabd5-c5f0-42d8-9065-395cebf8c29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = \"--packages=org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0 pyspark-shell\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecc9e27-31a9-4348-92f8-649036eb205d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'findspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfindspark\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'findspark'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import uuid\n",
    "import random\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import requests\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2525fbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"nomic-ai/nomic-embed-text-v1\", trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617af0ae-48da-405b-a806-8de69e662d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName('RealtimeKafkaML') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f30894-4089-4949-a3e4-3c1cf48c3b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = spark \\\n",
    "  .readStream \\\n",
    "  .format('kafka') \\\n",
    "  .option('kafka.bootstrap.servers', \"broker:29092\") \\\n",
    "  .option(\"startingOffsets\", \"latest\") \\\n",
    "  .option('subscribe', \"datipipe\") \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93be281a-ef52-4556-bc93-d109690c1035",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_json = df_raw.selectExpr('CAST(value AS STRING) as json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c671fb99-b4ef-4edf-b598-b190a1ffb009",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_schema = StructType([\n",
    "        StructField(\"url\", StringType(), True),\n",
    "        StructField(\"publishedAt\", StringType(), True),\n",
    "        StructField(\"description\", StringType(), True),\n",
    "        StructField(\"source\", StructType([\n",
    "            StructField(\"name\", StringType(), True),\n",
    "            StructField(\"id\", StringType(), True)\n",
    "        ]), True),\n",
    "        StructField(\"title\", StringType(), True),\n",
    "        StructField(\"urlToImage\", StringType(), True),\n",
    "        StructField(\"content\", StringType(), True),\n",
    "        StructField(\"author\", StringType(), True)\n",
    "    ])\n",
    "\n",
    "    # Definisci lo schema per l'intero JSON\n",
    "schema = StructType([\n",
    "    StructField(\"@timestamp\", StringType(), True),\n",
    "    StructField(\"articles\", article_schema, True),\n",
    "    StructField(\"@version\", StringType(), True),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "    StructField(\"totalResults\", StringType(), True)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28453a80-bdb8-4cc0-b0c4-cd3ef2e3f2e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'json'>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CustomEmbeddingFunction:\n",
    "    def __init__(self, ):\n",
    "        self.model = model\n",
    "\n",
    "    def __call__(self, input):\n",
    "        if isinstance(input, list):\n",
    "            return [self.generate_embeddings(text) for text in input]\n",
    "        else:\n",
    "            return [self.generate_embeddings(input)]\n",
    "\n",
    "    def generate_embeddings(self, text):\n",
    "        if text:\n",
    "            embeddings = self.model.encode([text], convert_to_tensor=False)\n",
    "            return embeddings.tolist()[0]\n",
    "        else:\n",
    "            return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46ea21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "# Creazione dell'istanza della classe di funzione di embedding\n",
    "embedding_function = CustomEmbeddingFunction()\n",
    "\n",
    "# Utilizzo dell'istanza per aggiungere testi\n",
    "client = chromadb.PersistentClient()\n",
    "collection = client.get_or_create_collection(name=\"test\", embedding_function=embedding_function)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee00405",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preparazione dei dati di testo\n",
    "sentences = [\"Who is Laurens van der Maaten?\", \"What is machine learning?\"]\n",
    "\n",
    "# Aggiunta dei testi e degli embeddings calcolati alla collezione\n",
    "collection.add(documents=sentences, ids = [\"id4\",\"id5\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6df3244",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0d16c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "def generate_sha256_hash_from_text(text) -> str:\n",
    "    # Create a SHA256 hash object\n",
    "    sha256_hash = hashlib.sha256()\n",
    "    # Update the hash object with the text encoded to bytes\n",
    "    sha256_hash.update(text.encode('utf-8'))\n",
    "    # Return the hexadecimal representation of the hash\n",
    "    return sha256_hash.hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8d7242",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_transformed = df_json.select(from_json(col(\"json\"), article_schema).alias('data'))\n",
    "#df_descriptions = df_transformed.select(\"data.articles.description\")\n",
    "import uuid\n",
    "\n",
    "# Definisci una funzione per inviare i dati a ChromaDB\n",
    "def send_to_chroma(batch_df, epoch_id, collection):\n",
    "    # Itera su ogni riga nel DataFrame del batch\n",
    "    for row in batch_df.collect():\n",
    "        document = row['content']  # Assumi che 'description' sia il campo di interesse\n",
    "        splits = text_splitter.split_text(document)\n",
    "        doc_id = str(uuid.uuid4())\n",
    "        unique = collection.get(ids = [generate_sha256_hash_from_text(splits[i]) for i in range(len(splits))])\n",
    "        if (unique[\"ids\"] != [] == True):\n",
    "            print(\"Testo gia presente\")\n",
    "        try:\n",
    "            # Invio del documento a ChromaDB\n",
    "            collection.add(documents=splits,ids=[doc_id])\n",
    "            #print(f\"\\rDocumento inviato a ChromaDB: {document}\",end = \"\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\rErrore nell'invio del documento a ChromaDB: {e}\",end = \"\")\n",
    "\n",
    "#query = df_descriptions.writeStream.foreachBatch(lambda df, epoch_id: send_to_chroma(df, epoch_id, collection)).start()\n",
    "\n",
    "#query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a825a086",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_to_chroma(batch_df, epoch_id, collection):\n",
    "    # Itera su ogni riga nel DataFrame del batch\n",
    "    for row in batch_df.collect():\n",
    "        document = row['content']  # Assumi che 'description' sia il campo di interesse\n",
    "        splits = text_splitter.split_text(document)\n",
    "        #doc_id = str(uuid.uuid4())\n",
    "        hashs = [generate_sha256_hash_from_text(splits[i]) for i in range(len(splits))]\n",
    "        unique = collection.get(ids = hashs )\n",
    "        bool_unique = unique[\"ids\"] != []\n",
    "        if (bool_unique == True):\n",
    "            print(\"Testo gia presente\")\n",
    "            return\n",
    "        try:\n",
    "            # Invio del documento a ChromaDB\n",
    "            collection.add(documents=splits,ids=hashs)\n",
    "            #print(f\"\\rDocumento inviato a ChromaDB: {document}\",end = \"\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\rErrore nell'invio del documento a ChromaDB: {e}\",end = \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8719c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_json.select(from_json(df_json.json, schema).alias('rowdata')) \\\n",
    "  .select('rowdata.articles.content') \\\n",
    "  .writeStream \\\n",
    "  .foreachBatch(lambda df, epoch_id: send_to_chroma(df, epoch_id, collection))\\\n",
    "  .start()\\\n",
    "  .awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8b30c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = collection.query(query_texts=[\"what is euclidian telescope\"])\n",
    "query "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45bc8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preparazione dei dati di testo\n",
    "sentences = [\"Who is Laurens van der Maaten?\", \"What is machine learning?\"]\n",
    "\n",
    "# Aggiunta dei testi e degli embeddings calcolati alla collezione\n",
    "collection.add(documents=sentences, ids = [generate_sha256_hash() for _ in range(len(sentences))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dd2db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"Who is Laurenssaas van der Maaten?\", \"What is machisdadane learning?\"]\n",
    "\n",
    "caxx = [\"Who is Laurens van der Maaten?\", \"What is machine learning?\"]\n",
    "hash2  = [generate_sha256_hash_from_text(caxx[i]) for i in range(len(caxx))]\n",
    "hash  = [generate_sha256_hash_from_text(sentences[i]) for i in range(len(sentences))]\n",
    "o = collection.get(ids = [generate_sha256_hash_from_text(sentences[i]) for i in range(len(sentences))])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
